{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T08:10:26.295564Z",
     "start_time": "2025-05-08T08:10:26.280153Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:34:29.039735Z",
     "start_time": "2025-05-07T14:34:28.007137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup and Imports\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(5)\n",
    "\n",
    "# Data Loading\n",
    "crtDir = os.getcwd()\n",
    "fileName = os.path.join(crtDir, 'data', 'spam.csv')\n",
    "\n",
    "data = []\n",
    "with open(fileName) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            dataNames = row\n",
    "            print(f\"Column names: {dataNames}\")\n",
    "        else:\n",
    "            data.append(row)\n",
    "        line_count += 1\n",
    "\n",
    "print(f\"Total number of rows: {line_count-1}\")\n",
    "\n",
    "# Mapping the collumns\n",
    "inputs = [data[i][0] for i in range(len(data))][:100]\n",
    "outputs = [data[i][1] for i in range(len(data))][:100]\n",
    "\n",
    "# Show a few examples\n",
    "print(\"\\nFirst 5 messages and their labels:\")\n",
    "for i in range(5):\n",
    "    print(f\"Label: {outputs[i]}\")\n",
    "    print(f\"Message: {inputs[i][:100]}...\" if len(inputs[i]) > 100 else f\"Message: {inputs[i]}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "de9b8b8996748885",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: ['emailText', 'emailType']\n",
      "Total number of rows: 5572\n",
      "\n",
      "First 5 messages and their labels:\n",
      "Label: ham\n",
      "Message: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got a...\n",
      "--------------------------------------------------\n",
      "Label: ham\n",
      "Message: Ok lar... Joking wif u oni...\n",
      "--------------------------------------------------\n",
      "Label: spam\n",
      "Message: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entr...\n",
      "--------------------------------------------------\n",
      "Label: ham\n",
      "Message: U dun say so early hor... U c already then say...\n",
      "--------------------------------------------------\n",
      "Label: ham\n",
      "Message: Nah I don't think he goes to usf, he lives around here though\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Train and Split Data",
   "id": "9760a390b523477"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:34:39.041348Z",
     "start_time": "2025-05-07T14:34:38.963726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens), tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "cleaned_inputs = [preprocess_text(text) for text in inputs]\n",
    "cleaned_texts = [item[0] for item in cleaned_inputs]\n",
    "tokenized_texts = [item[1] for item in cleaned_inputs]\n",
    "\n",
    "# Split into train and test\n",
    "noSamples = len(inputs)\n",
    "indexes = list(range(noSamples))\n",
    "trainSample = np.random.choice(indexes, int(0.8 * noSamples), replace=False)\n",
    "testSample = [i for i in indexes if i not in trainSample]\n",
    "\n",
    "trainInputs = [cleaned_texts[i] for i in trainSample]\n",
    "trainTokens = [tokenized_texts[i] for i in trainSample]\n",
    "trainOutputs = [outputs[i] for i in trainSample]\n",
    "testInputs = [cleaned_texts[i] for i in testSample]\n",
    "testTokens = [tokenized_texts[i] for i in testSample]\n",
    "testOutputs = [outputs[i] for i in testSample]\n",
    "\n",
    "print(f\"Training set size: {len(trainInputs)}\")\n",
    "print(f\"Test set size: {len(testInputs)}\")\n",
    "\n",
    "# Class distribution\n",
    "train_spam_count = sum(1 for label in trainOutputs if label == \"spam\")\n",
    "train_ham_count = len(trainOutputs) - train_spam_count\n",
    "test_spam_count = sum(1 for label in testOutputs if label == \"spam\")\n",
    "test_ham_count = len(testOutputs) - test_spam_count\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"Training set: {train_spam_count} spam, {train_ham_count} ham\")\n",
    "print(f\"Test set: {test_spam_count} spam, {test_ham_count} ham\")"
   ],
   "id": "d23ce680b814ad00",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 80\n",
      "Test set size: 20\n",
      "\n",
      "Class distribution:\n",
      "Training set: 14 spam, 66 ham\n",
      "Test set: 3 spam, 17 ham\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "BAG OF WORDS",
   "id": "ec395516979a850e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:41:57.857006Z",
     "start_time": "2025-05-07T14:41:57.834492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Bag of Words\n",
    "print(\"\\n=== Bag of Words ===\")\n",
    "vectorizer_bow = CountVectorizer(max_features=1000)\n",
    "trainFeatures_bow = vectorizer_bow.fit_transform(trainInputs)\n",
    "testFeatures_bow = vectorizer_bow.transform(testInputs)\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Vocabulary size: {len(vectorizer_bow.vocabulary_)} words\")\n",
    "print(f\"Training features shape: {trainFeatures_bow.shape}\")\n",
    "\n",
    "# Sample vocabulary\n",
    "vocab_sample = list(vectorizer_bow.vocabulary_.keys())[:20]\n",
    "print(\"\\nSample words from vocabulary:\")\n",
    "print(vocab_sample)\n",
    "\n",
    "\n",
    "# Sparsity\n",
    "total_elements = trainFeatures_bow.shape[0] * trainFeatures_bow.shape[1]\n",
    "non_zero_elements = trainFeatures_bow.nnz\n",
    "sparsity = 100 * (1 - non_zero_elements / total_elements)\n",
    "print(f\"\\nSparsity of training feature matrix: {sparsity:.2f}%\")\n",
    "\n",
    "# Average number of non-zero features per sample\n",
    "avg_non_zero = non_zero_elements / trainFeatures_bow.shape[0]\n",
    "print(f\"Average number of non-zero features per email: {avg_non_zero:.2f}\")\n",
    "\n",
    "# Top 10 most frequent words\n",
    "word_counts = np.array(trainFeatures_bow.sum(axis=0)).flatten()\n",
    "vocab = list(vectorizer_bow.vocabulary_.keys())\n",
    "word_freq = [(vocab[i], word_counts[i]) for i in range(len(vocab))]\n",
    "word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 most frequent words:\")\n",
    "for word, freq in word_freq:\n",
    "    print(f\"  '{word}': {freq}\")\n"
   ],
   "id": "b1145d86c1ada686",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bag of Words ===\n",
      "Vocabulary size: 524 words\n",
      "Training features shape: (80, 524)\n",
      "\n",
      "Sample words from vocabulary:\n",
      "['today', 'song', 'dedicated', 'day', 'dedicate', 'send', 'ur', 'valuable', 'frnds', 'first', 'rply', 'tell', 'anything', 'didnt', 'get', 'hep', 'immunisation', 'nigeria', 'im', 'back']\n",
      "\n",
      "Sparsity of training feature matrix: 98.29%\n",
      "Average number of non-zero features per email: 8.97\n",
      "\n",
      "Top 10 most frequent words:\n",
      "  'babe': 10\n",
      "  'letter': 9\n",
      "  'trying': 9\n",
      "  'text': 8\n",
      "  'breather': 7\n",
      "  'dont': 6\n",
      "  'pobox': 6\n",
      "  'ta': 6\n",
      "  'devils': 6\n",
      "  'cried': 5\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TF-IDF",
   "id": "f398b6d4edee3ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:38:18.651281Z",
     "start_time": "2025-05-07T14:38:18.608650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TF-IDF\n",
    "print(\"\\n=== TF-IDF ===\")\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=1000)\n",
    "trainFeatures_tfidf = vectorizer_tfidf.fit_transform(trainInputs)\n",
    "testFeatures_tfidf = vectorizer_tfidf.transform(testInputs)\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Vocabulary size: {len(vectorizer_tfidf.vocabulary_)} words\")\n",
    "print(f\"Training features shape: {trainFeatures_tfidf.shape}\")\n",
    "\n",
    "# Sample vocabulary\n",
    "vocab_sample = list(vectorizer_tfidf.vocabulary_.keys())[:20]\n",
    "print(\"\\nSample words from vocabulary:\")\n",
    "print(vocab_sample)\n",
    "\n",
    "# Additional stats\n",
    "# Sparsity\n",
    "total_elements = trainFeatures_tfidf.shape[0] * trainFeatures_tfidf.shape[1]\n",
    "non_zero_elements = trainFeatures_tfidf.nnz\n",
    "sparsity = 100 * (1 - non_zero_elements / total_elements)\n",
    "print(f\"\\nSparsity of training feature matrix: {sparsity:.2f}%\")\n",
    "\n",
    "# Average number of non-zero features per sample\n",
    "avg_non_zero = non_zero_elements / trainFeatures_tfidf.shape[0]\n",
    "print(f\"Average number of non-zero features per email: {avg_non_zero:.2f}\")\n",
    "\n",
    "# Top 10 words by TF-IDF sum\n",
    "tfidf_sums = np.array(trainFeatures_tfidf.sum(axis=0)).flatten()\n",
    "vocab = list(vectorizer_tfidf.vocabulary_.keys())\n",
    "word_tfidf = [(vocab[i], tfidf_sums[i]) for i in range(len(vocab))]\n",
    "word_tfidf = sorted(word_tfidf, key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"\\nTop 10 words by TF-IDF sum:\")\n",
    "for word, score in word_tfidf:\n",
    "    print(f\"  '{word}': {score:.2f}\")\n",
    "\n"
   ],
   "id": "fd6050e920c291cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TF-IDF ===\n",
      "Vocabulary size: 524 words\n",
      "Training features shape: (80, 524)\n",
      "\n",
      "Sample words from vocabulary:\n",
      "['today', 'song', 'dedicated', 'day', 'dedicate', 'send', 'ur', 'valuable', 'frnds', 'first', 'rply', 'tell', 'anything', 'didnt', 'get', 'hep', 'immunisation', 'nigeria', 'im', 'back']\n",
      "\n",
      "Sparsity of training feature matrix: 98.29%\n",
      "Average number of non-zero features per email: 8.97\n",
      "\n",
      "Top 10 words by TF-IDF sum:\n",
      "  'letter': 3.11\n",
      "  'cried': 2.09\n",
      "  'babe': 2.07\n",
      "  'text': 1.83\n",
      "  'trying': 1.68\n",
      "  'apologetic': 1.64\n",
      "  'ta': 1.49\n",
      "  'devils': 1.43\n",
      "  'moviewat': 1.42\n",
      "  'breather': 1.38\n",
      "\n",
      "L2 norm of feature vectors:\n",
      "  Mean: 1.00\n",
      "  Std: 0.00\n",
      "  Min: 1.00\n",
      "  Max: 1.00\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4f43736353a6fe2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T15:14:48.916194Z",
     "start_time": "2025-05-07T15:14:48.802236Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8685d8e7ff8fe3e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Word2Vec ===\n",
      "Word2Vec vocabulary size: 534\n",
      "Training features shape: (80, 100)\n",
      "\n",
      "Average number of words per email used for vectors: 10.00\n",
      "Std of words per email: 6.10\n",
      "\n",
      "Word similarities for spam-related words:\n",
      "\n",
      "Words similar to 'free':\n",
      "  congrats: 0.2958\n",
      "  ï¿½5month: 0.2724\n",
      "  gram: 0.2555\n",
      "  ratetcs: 0.2378\n",
      "  etc: 0.2368\n",
      "\n",
      "Words similar to 'win':\n",
      "  entry: 0.3167\n",
      "  looking: 0.2590\n",
      "  go: 0.2375\n",
      "  invite: 0.2351\n",
      "  str: 0.2340\n",
      "\n",
      "Words similar to 'urgent':\n",
      "  gon: 0.2671\n",
      "  chgs: 0.2635\n",
      "  xuhui: 0.2500\n",
      "  song: 0.2372\n",
      "  trav: 0.2347\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Additional Feature Extraction",
   "id": "d6e4d9c1849805c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T15:17:41.562672Z",
     "start_time": "2025-05-07T15:17:41.457862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Additional Features\n",
    "print(\"\\n=== Additional Features ===\")\n",
    "def extract_additional_features(texts):\n",
    "    features = []\n",
    "    for text in texts:\n",
    "        length = len(text)\n",
    "        punctuation_count = sum(1 for char in text if char in string.punctuation)\n",
    "        token_count = len(word_tokenize(text))\n",
    "        features.append([length, punctuation_count, token_count])\n",
    "    return np.array(features)\n",
    "\n",
    "trainFeatures_additional = extract_additional_features([inputs[i] for i in trainSample])\n",
    "testFeatures_additional = extract_additional_features([inputs[i] for i in testSample])\n",
    "print(f\"Additional features shape: {trainFeatures_additional.shape} (length, punctuation, token count)\")\n",
    "\n",
    "\n",
    "# Statistics for each feature\n",
    "feature_names = [\"Message Length\", \"Punctuation Count\", \"Token Count\"]\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean: {trainFeatures_additional[:, i].mean():.2f}\")\n",
    "    print(f\"  Std: {trainFeatures_additional[:, i].std():.2f}\")\n",
    "    print(f\"  Min: {trainFeatures_additional[:, i].min():.2f}\")\n",
    "    print(f\"  Max: {trainFeatures_additional[:, i].max():.2f}\")\n",
    "\n",
    "# Feature differences between spam and ham\n",
    "spam_mask = np.array(trainOutputs) == \"spam\"\n",
    "ham_mask = np.array(trainOutputs) == \"ham\"\n",
    "print(\"\\nFeature differences between spam and ham:\")\n",
    "for i, name in enumerate(feature_names):\n",
    "    spam_mean = trainFeatures_additional[spam_mask, i].mean() if spam_mask.sum() > 0 else 0\n",
    "    ham_mean = trainFeatures_additional[ham_mask, i].mean() if ham_mask.sum() > 0 else 0\n",
    "    print(f\"  {name}: Spam mean = {spam_mean:.2f}, Ham mean = {ham_mean:.2f}\")"
   ],
   "id": "e49f3dd90e74a406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Additional Features ===\n",
      "Additional features shape: (80, 3) (length, punctuation, token count)\n",
      "\n",
      "Message Length:\n",
      "  Mean: 85.89\n",
      "  Std: 51.82\n",
      "  Min: 14.00\n",
      "  Max: 196.00\n",
      "\n",
      "Punctuation Count:\n",
      "  Mean: 4.24\n",
      "  Std: 3.18\n",
      "  Min: 0.00\n",
      "  Max: 15.00\n",
      "\n",
      "Token Count:\n",
      "  Mean: 20.19\n",
      "  Std: 11.65\n",
      "  Min: 4.00\n",
      "  Max: 48.00\n",
      "\n",
      "Feature differences between spam and ham:\n",
      "  Message Length: Spam mean = 144.93, Ham mean = 73.36\n",
      "  Punctuation Count: Spam mean = 6.36, Ham mean = 3.79\n",
      "  Token Count: Spam mean = 30.71, Ham mean = 17.95\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Combination and Analysis",
   "id": "5577480538bdb7b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T15:19:35.400107Z",
     "start_time": "2025-05-07T15:19:35.253520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\n=== Classification Results ===\")\n",
    "def evaluate_features(features_train, features_test, train_labels, test_labels, name):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(features_train, train_labels)\n",
    "    pred = clf.predict(features_test)\n",
    "    accuracy = accuracy_score(test_labels, pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "evaluate_features(trainFeatures_bow, testFeatures_bow, trainOutputs, testOutputs, \"BoW\")\n",
    "evaluate_features(trainFeatures_tfidf, testFeatures_tfidf, trainOutputs, testOutputs, \"TF-IDF\")\n",
    "evaluate_features(trainFeatures_w2v, testFeatures_w2v, trainOutputs, testOutputs, \"Word2Vec\")\n",
    "evaluate_features(trainFeatures_additional, testFeatures_additional, trainOutputs, testOutputs, \"Additional Features\")"
   ],
   "id": "6421a88fd14f19e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Results ===\n",
      "BoW Accuracy: 0.8500\n",
      "TF-IDF Accuracy: 0.8500\n",
      "Word2Vec Accuracy: 0.8500\n",
      "Additional Features Accuracy: 0.8500\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
